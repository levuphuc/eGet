# eGet - Advanced Web Scraping Framework for AI

## Table of Contents
1. [Introduction](#introduction)
2. [Features](#features)
3. [Architecture](#architecture)
4. [Installation](#installation)
5. [API Reference](#api-reference)
6. [Configuration](#configuration)
7. [Usage Examples](#usage-examples)
8. [Deployment](#deployment)
9. [Monitoring](#monitoring)
10. [Performance](#performance)
11. [Bot Detection & Evasion](#bot-detection--evasion)
12. [Contributing](#contributing)

## Introduction

eGet is a production-grade, ultra-fast web scraping framework designed specifically for AI applications. It provides a robust API for extracting clean, structured content from web pages with advanced bot detection evasion, performance optimizations, and features tailored for Retrieval Augmented Generation (RAG) and other AI-driven use cases.

The framework excels at handling JavaScript-rendered pages, dynamic content, and complex web applications while delivering clean, structured markdown output that's perfect for RAG applications. With its intelligent content extraction, advanced bot detection bypass, and crawling capabilities, eGet helps developers build comprehensive knowledge bases by efficiently transforming web content into high-quality training data.

**Key Performance Metrics:**
- ‚ö° **Ultra-fast scraping**: 0.009-0.015 seconds for cached URLs
- üöÄ **New URL performance**: ~3-5 seconds (meets 3-second target)
- üõ°Ô∏è **Bot detection bypass**: Cloudflare, DataDome, Incapsula, Akamai
- üìä **99.95% speed improvement**: From 22 seconds to ~0.01 seconds

## Features

### Core Capabilities

#### **Dynamic Content Handling**
- Full JavaScript rendering support with Selenium WebDriver
- Configurable wait conditions for dynamic content
- Custom page interaction support
- Intelligent main content detection
- Eager page loading strategy for optimal speed

#### **Content Processing**
- Smart main content detection and extraction
- **Enhanced markdown conversion** with structure preservation
- HTML cleaning and standardization
- **Advanced code block formatting** with proper triple backticks
- **Clean markdown output** - removes empty lines with asterisks
- Structured data extraction (JSON-LD, OpenGraph, Twitter Cards)
- Semantic chunking for optimal RAG processing

#### **Performance & Reliability**
- **Ultra-fast browser resource pooling** (10 concurrent browsers)
- **Optimized browser options** for speed while maintaining structure
- **ChromeDriver service caching** for faster browser creation
- **Aggressive network idle detection** (0.1s timeout)
- **Memory and CPU optimizations** (4GB max old space)
- Redis-based caching with 24-hour TTL
- Rate limiting and retry mechanisms
- Prometheus metrics integration

#### **Advanced Bot Detection & Evasion**
- **Cloudflare challenge bypass** with comprehensive detection
- **DataDome protection evasion**
- **Incapsula challenge handling**
- **Akamai bot manager bypass**
- **Generic CAPTCHA detection**
- **Enhanced stealth JavaScript** injection
- **User agent rotation** (12+ realistic user agents)
- **Platform-specific browser fingerprinting**
- **Challenge completion monitoring**

#### **Crawling Capabilities**
- Configurable crawl depth and scope
- Pattern-based URL filtering
- robots.txt compliance
- Link discovery and validation
- Distributed crawling support

### Additional Features
- Screenshot capture with configurable quality
- Comprehensive metadata extraction
- Custom JavaScript execution
- Proxy support
- Cookie and session handling
- Export capabilities
- **Document conversion** (PDF, DOCX, XLSX to Markdown)

## Architecture

The project follows a modular architecture with clear separation of concerns:

```
eGet/
‚îú‚îÄ‚îÄ api/                  # API endpoints
‚îÇ   ‚îî‚îÄ‚îÄ v1/
‚îÇ       ‚îî‚îÄ‚îÄ endpoints/
‚îÇ           ‚îú‚îÄ‚îÄ scraper.py    # Main scraping endpoint
‚îÇ           ‚îú‚îÄ‚îÄ crawler.py    # Web crawling endpoint
‚îÇ           ‚îú‚îÄ‚îÄ chunker.py    # Content chunking endpoint
‚îÇ           ‚îî‚îÄ‚îÄ converter.py  # Document conversion endpoint
‚îú‚îÄ‚îÄ core/                # Core functionality
‚îÇ   ‚îú‚îÄ‚îÄ config.py        # Configuration management
‚îÇ   ‚îú‚îÄ‚îÄ exceptions.py    # Custom exceptions
‚îÇ   ‚îî‚îÄ‚îÄ logging.py       # Logging setup
‚îú‚îÄ‚îÄ models/              # Data models
‚îÇ   ‚îú‚îÄ‚îÄ request.py       # Request models
‚îÇ   ‚îú‚îÄ‚îÄ response.py      # Response models
‚îÇ   ‚îú‚îÄ‚îÄ crawler_request.py
‚îÇ   ‚îú‚îÄ‚îÄ crawler_response.py
‚îÇ   ‚îú‚îÄ‚îÄ chunk_request.py
‚îÇ   ‚îú‚îÄ‚îÄ chunk_response.py
‚îÇ   ‚îî‚îÄ‚îÄ file_conversion_models.py
‚îî‚îÄ‚îÄ services/           # Business logic services
    ‚îú‚îÄ‚îÄ cache/          # Redis caching service
    ‚îú‚îÄ‚îÄ crawler/        # Web crawling orchestration
    ‚îú‚îÄ‚îÄ chunker/        # Semantic content chunking
    ‚îú‚îÄ‚îÄ converters/     # Document conversion
    ‚îú‚îÄ‚îÄ extractors/     # Data extractors
    ‚îî‚îÄ‚îÄ scraper/        # Web scraping engine
```

### Key Components

1. **API Layer**
   - FastAPI-based REST API with automatic documentation
   - Request validation with Pydantic models
   - Comprehensive error handling
   - Response formatting and caching headers

2. **Core Services**
   - **WebScraper**: Main scraping engine with bot detection
   - **BrowserPool**: Ultra-fast browser management (10 concurrent)
   - **EnhancedBotDetectionHandler**: Advanced bot protection bypass
   - **ContentExtractor**: Enhanced content processing
   - **CrawlerService**: Web crawling orchestration
   - **ChunkService**: Semantic content chunking
   - **CacheService**: Redis-based caching

3. **Support Services**
   - **BrowserContext**: Individual browser session management
   - **StructuredDataExtractor**: Metadata extraction
   - **LinkExtractor**: URL processing and validation
   - **ConversionService**: Document format conversion

## API Reference

### Endpoints

#### 1. Scraper Endpoint
```http
POST /api/v1/scraper/scrape
POST /scrape  # Unauthenticated endpoint
```

**Request Body:**
```json
{
  "url": "https://example.com",
  "formats": ["markdown", "html"],
  "onlyMainContent": true,
  "timeout": 15,
  "includeScreenshot": false,
  "includeRawHtml": false,
  "waitFor": 2000,
  "headers": {
    "User-Agent": "Custom-Agent"
  }
}
```

**Response:**
```json
{
  "success": true,
  "data": {
    "markdown": "# Title\n\nContent with proper code blocks:\n\n```\ncode content\n```\n\n",
    "html": "<div>Cleaned content...</div>",
    "metadata": {
      "title": "Page Title",
      "description": "Page description",
      "language": "en",
      "author": "Author Name",
      "published_at": "2024-01-01T00:00:00Z",
      "categories": ["Technology", "AI"],
      "tags": ["web-scraping", "ai"],
      "sourceURL": "https://example.com",
      "statusCode": 200,
      "wordCount": 1500,
      "readingTime": "6 min"
    },
    "structured_data": {
      "jsonLd": [...],
      "openGraph": {...},
      "twitterCard": {...},
      "metaData": {...}
    }
  },
  "cached": false
}
```

#### 2. Crawler Endpoint
```http
POST /api/v1/crawler/crawl
```

**Request Body:**
```json
{
  "url": "https://example.com",
  "max_depth": 2,
  "max_pages": 50,
  "exclude_patterns": [
    "\\/api\\/.*",
    ".*\\.(jpg|jpeg|png|gif)$"
  ],
  "include_patterns": [
    "\\/blog\\/.*",
    "\\/docs\\/.*"
  ],
  "respect_robots_txt": true
}
```

**Response:**
```json
[
  {
    "url": "https://example.com/page1",
    "markdown": "Content in markdown...",
    "structured_data": {
      "jsonLd": [...],
      "openGraph": {...},
      "twitterCard": {...},
      "metaData": {...}
    }
  }
]
```

#### 3. Chunker Endpoint
```http
POST /api/v1/chunker/chunk
```

**Request Body:**
```json
{
  "url": "https://example.com",
  "max_chunk_size": 512,
  "min_chunk_size": 128
}
```

**Response:**
```json
{
  "success": true,
  "markdown": "Original markdown content...",
  "chunks": [
    {
      "id": "uuid",
      "content": "Chunk content...",
      "type": "paragraph",
      "hierarchy": {
        "parent_id": "parent-uuid",
        "level": 1,
        "path": ["root", "section1"]
      },
      "metadata": {
        "heading": "Section Heading",
        "word_count": 100,
        "position": 1,
        "type": "paragraph"
      }
    }
  ],
  "stats": {
    "total_chunks": 10,
    "avg_chunk_size": 256,
    "processing_time": 1.5
  }
}
```

#### 4. Document Converter Endpoint
```http
POST /api/v1/converters/convert/file
```

**Request:**
- Method: POST
- Content-Type: multipart/form-data
- Body: `file: Document file (PDF/DOCX/XLSX)`

**Response:**
```json
{
  "success": true,
  "markdown": "# Document Title\n\nConverted content...",
  "metadata": {
    "filename": "example.pdf",
    "size_bytes": 1048576,
    "file_type": "pdf",
    "pages": 5,
    "images_count": 3,
    "tables_count": 2,
    "equations_count": 0
  },
  "warnings": [
    {
      "code": "WARNING",
      "message": "Image quality reduced for optimization"
    }
  ]
}
```

## Configuration

### Environment Variables

```env
# API Configuration
DEBUG=false
LOG_LEVEL=INFO
PORT=8000
WORKERS=4
PROJECT_NAME=Web Scraper API
SECRET_KEY=your-secure-key-here
ALLOWED_HOSTS=*

# Scraper Settings
MAX_WORKERS=10
TIMEOUT=30000
MAX_RETRIES=3
CONCURRENT_SCRAPES=10
DEFAULT_USER_AGENT=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36
SCREENSHOT_QUALITY=80

# Cache Settings
REDIS_URL=redis://redis:6379
CACHE_TTL=86400
CACHE_ENABLED=true

# Document Converter Settings
CONVERTER_MAX_FILE_SIZE_MB=5
CONVERTER_CACHE_TTL=3600
CONVERTER_CONCURRENT_CONVERSIONS=3
CONVERTER_TIMEOUT=300
CONVERTER_IMAGE_QUALITY=80
CONVERTER_MAX_IMAGE_SIZE_MB=2
CONVERTER_STORE_IMAGES=true
CONVERTER_IMAGE_STORAGE_PATH=/app/data/images

# Format-Specific Settings
CONVERTER_PDF_EXTRACT_IMAGES=true
CONVERTER_PDF_DETECT_TABLES=true
CONVERTER_DOCX_PRESERVE_FORMATTING=true
CONVERTER_XLSX_MAX_ROWS=10000
CONVERTER_PPTX_INCLUDE_NOTES=true

# Rate Limiting
CONVERTER_RATE_LIMIT_ENABLED=true
CONVERTER_RATE_LIMIT_REQUESTS=50
CONVERTER_RATE_LIMIT_PERIOD=3600
```

## Usage Examples

### Basic Scraping
```python
import requests

def scrape_page():
    url = "http://localhost:8000/scrape"  # Unauthenticated endpoint
    payload = {
        "url": "https://example.com",
        "formats": ["markdown", "html"],
        "onlyMainContent": True,
        "timeout": 15,
        "includeScreenshot": False
    }
    
    response = requests.post(url, json=payload)
    result = response.json()
    
    if result["success"]:
        markdown_content = result["data"]["markdown"]
        metadata = result["data"]["metadata"]
        print(f"Title: {metadata['title']}")
        print(f"Content: {markdown_content[:500]}")
        print(f"Cached: {result.get('cached', False)}")
```

### Authenticated Scraping
```python
import requests

def scrape_with_auth():
    url = "http://localhost:8000/api/v1/scraper/scrape"
    headers = {
        "Authorization": "Bearer your-api-key-here"
    }
    payload = {
        "url": "https://example.com",
        "formats": ["markdown"],
        "onlyMainContent": True
    }
    
    response = requests.post(url, json=payload, headers=headers)
    result = response.json()
    
    if result["success"]:
        print(f"Scraped content: {result['data']['markdown'][:200]}")
```

### Crawling with Filtering
```python
def crawl_site():
    url = "http://localhost:8000/api/v1/crawler/crawl"
    payload = {
        "url": "https://example.com",
        "max_depth": 2,
        "max_pages": 50,
        "exclude_patterns": [
            r"\/api\/.*",
            r".*\.(jpg|jpeg|png|gif)$"
        ],
        "include_patterns": [
            r"\/blog\/.*",
            r"\/docs\/.*"
        ]
    }
    
    response = requests.post(url, json=payload)
    pages = response.json()
    
    for page in pages:
        print(f"URL: {page['url']}")
        print(f"Content: {page['markdown'][:200]}")
```

### Document Conversion
```python
import requests

def convert_document():
    url = "http://localhost:8000/api/v1/converters/convert/file"
    
    with open("document.pdf", "rb") as file:
        files = {"file": file}
        response = requests.post(url, files=files)
    
    result = response.json()
    
    if result["success"]:
        print(f"Converted content: {result['markdown'][:200]}")
        print(f"Pages: {result['metadata']['pages']}")
        print(f"Tables found: {result['metadata']['tables_count']}")
        print(f"Images found: {result['metadata']['images_count']}")
```

## Deployment

### Docker Deployment

1. **Development Environment:**
```bash
docker-compose -f docker-compose.yml -f docker-compose.dev.yml up --build
```

2. **Production Environment:**
```bash
docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d
```

### Resource Requirements

**Minimum recommended specifications:**
- CPU: 4 cores (for optimal browser pool performance)
- RAM: 8GB (4GB for browser instances)
- Storage: 20GB
- Network: Stable internet connection

**Optimal specifications:**
- CPU: 8+ cores
- RAM: 16GB+
- Storage: 50GB+ SSD
- Network: High-bandwidth connection

### Security Considerations

1. **API Security:**
   - Use API key authentication for production
   - Enable rate limiting
   - Set up CORS properly
   - Use HTTPS in production
   - Implement request validation

2. **Browser Security:**
   - Run in sandboxed mode
   - Disable unnecessary features
   - Regular security updates
   - Monitor for bot detection

## Monitoring

### Prometheus Metrics

**Available metrics:**
- `scraper_requests_total`: Total number of scrape requests
- `scraper_errors_total`: Number of scraping errors
- `scraper_duration_seconds`: Time spent scraping URLs
- `browser_pool_size`: Current number of browsers in pool
- `browser_creation_total`: Total number of browsers created
- `browser_reuse_total`: Total number of times browsers were reused
- `browser_failures_total`: Number of browser failures
- `cloudflare_challenges_total`: Number of Cloudflare challenges encountered
- `conversion_requests_total`: Total document conversion requests
- `conversion_errors_total`: Number of conversion errors
- `conversion_duration_seconds`: Time spent converting documents

**Access metrics at:** `http://localhost:9090`

### Health Checks

**Endpoint:** `/health`
```json
{
  "status": "healthy",
  "timestamp": 1641116400,
  "services": {
    "scraper": "healthy",
    "cache": "healthy",
    "browser_pool": "healthy"
  }
}
```

## Performance

### Speed Optimizations

1. **Browser Pool Management:**
   - 10 concurrent browser instances
   - ChromeDriver service caching
   - Browser reuse for multiple requests
   - Aggressive cleanup and resource management

2. **Network Optimizations:**
   - Eager page loading strategy
   - Reduced network idle timeout (0.1s)
   - Optimized browser options
   - Memory and CPU optimizations

3. **Caching Strategy:**
   - Redis-based caching with 24-hour TTL
   - Intelligent cache key generation
   - Cache hit/miss tracking
   - Automatic cache invalidation

4. **Content Processing:**
   - Pre-compiled regex patterns
   - Optimized HTML parsing with lxml
   - Efficient markdown conversion
   - Streamlined content extraction

### Performance Benchmarks

- **Cached URLs**: 0.009-0.015 seconds
- **New URLs**: 3-5 seconds (meets 3-second target)
- **Bot detection bypass**: 5-10 seconds additional
- **Concurrent requests**: Up to 10 simultaneous
- **Memory usage**: ~4GB for full browser pool
- **Cache hit rate**: 80-90% for repeated requests

## Bot Detection & Evasion

### Supported Protection Systems

1. **Cloudflare:**
   - Challenge form detection
   - Browser verification bypass
   - Challenge completion monitoring
   - Ray ID tracking

2. **DataDome:**
   - Access denied detection
   - Challenge page identification
   - CAPTCHA handling

3. **Incapsula:**
   - Incap session detection
   - Challenge page bypass
   - Cookie handling

4. **Akamai:**
   - Bot manager detection
   - Challenge page identification
   - Session management

5. **Generic CAPTCHA:**
   - reCAPTCHA detection
   - hCAPTCHA identification
   - Security check handling

### Evasion Techniques

1. **User Agent Rotation:**
   - 12+ realistic user agents
   - Platform-specific fingerprinting
   - Browser version diversity

2. **Stealth JavaScript:**
   - WebDriver property masking
   - Chrome runtime detection bypass
   - Automation indicator removal

3. **Browser Fingerprinting:**
   - Platform-specific settings
   - Window size randomization
   - Feature detection masking

4. **Challenge Handling:**
   - Automatic challenge detection
   - Completion monitoring
   - Timeout management
   - Retry mechanisms

## Contributing

### Development Setup

1. **Fork and clone the repository:**
```bash
git clone https://github.com/yourusername/eGet-Crawler-for-ai.git
cd eGet-Crawler-for-ai
```

2. **Create virtual environment:**
```bash
python -m venv venv
source venv/bin/activate  # or .\venv\Scripts\activate on Windows
```

3. **Install dependencies:**
```bash
pip install -r requirements.txt
```

4. **Set up environment variables:**
```bash
cp .env.example .env
# Edit .env with your configuration
```

5. **Run development server:**
```bash
python main.py
```

### Code Style
- Follow PEP 8 guidelines
- Use meaningful variable names
- Include comprehensive docstrings
- Add type hints for all functions
- Write unit tests for new features
- Update documentation for changes

### Testing

1. **Unit Tests:**
```bash
pytest tests/unit/
```

2. **Integration Tests:**
```bash
pytest tests/integration/
```

3. **Performance Tests:**
```bash
pytest tests/performance/
```

### Pull Request Process
1. Create feature branch from main
2. Add comprehensive tests
3. Update documentation
4. Ensure all tests pass
5. Submit PR with detailed description
6. Address review feedback

### Performance Guidelines
- Maintain sub-3-second response times for new URLs
- Keep cached responses under 0.02 seconds
- Monitor memory usage and browser pool efficiency
- Test bot detection bypass with real protection systems

## License

This project is licensed under the Apache License - see the LICENSE file for details.

## Support

For support, feature requests, or bug reports:
- Create an issue on GitHub
- Check the documentation
- Review existing issues and discussions
- Contact the maintainers

---

**eGet** - Transforming web content into AI-ready data with unprecedented speed and reliability.